2023-12-12 17:51:43.236626: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-12 17:51:43.239120: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-12-12 17:51:43.280746: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-12 17:51:43.280771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-12 17:51:43.282454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-12 17:51:43.289749: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-12-12 17:51:43.290008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-12 17:51:45.380119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Model: "3dcnn"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 128, 128, 64, 1   0         
                             )]                                  
                                                                 
 conv3d (Conv3D)             (None, 126, 126, 62, 64   1792      
                             )                                   
                                                                 
 max_pooling3d (MaxPooling3  (None, 63, 63, 31, 64)    0         
 D)                                                              
                                                                 
 batch_normalization (Batch  (None, 63, 63, 31, 64)    256       
 Normalization)                                                  
                                                                 
 conv3d_1 (Conv3D)           (None, 61, 61, 29, 64)    110656    
                                                                 
 max_pooling3d_1 (MaxPoolin  (None, 30, 30, 14, 64)    0         
 g3D)                                                            
                                                                 
 batch_normalization_1 (Bat  (None, 30, 30, 14, 64)    256       
 chNormalization)                                                
                                                                 
 conv3d_2 (Conv3D)           (None, 28, 28, 12, 128)   221312    
                                                                 
 max_pooling3d_2 (MaxPoolin  (None, 14, 14, 6, 128)    0         
 g3D)                                                            
                                                                 
 batch_normalization_2 (Bat  (None, 14, 14, 6, 128)    512       
 chNormalization)                                                
                                                                 
 conv3d_3 (Conv3D)           (None, 12, 12, 4, 256)    884992    
                                                                 
 max_pooling3d_3 (MaxPoolin  (None, 6, 6, 2, 256)      0         
 g3D)                                                            
                                                                 
 batch_normalization_3 (Bat  (None, 6, 6, 2, 256)      1024      
 chNormalization)                                                
                                                                 
 global_average_pooling3d (  (None, 256)               0         
 GlobalAveragePooling3D)                                         
                                                                 
 dense (Dense)               (None, 512)               131584    
                                                                 
 dropout (Dropout)           (None, 512)               0         
                                                                 
 dense_1 (Dense)             (None, 1)                 513       
                                                                 
=================================================================
Total params: 1352897 (5.16 MB)
Trainable params: 1351873 (5.16 MB)
Non-trainable params: 1024 (4.00 KB)
_________________________________________________________________
Epoch 1/100
/home/lodhar/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
144/144 - 58s - loss: 0.7247 - acc: 0.4896 - val_loss: 0.7205 - val_acc: 0.4790 - 58s/epoch - 400ms/step
Epoch 2/100
144/144 - 55s - loss: 0.7060 - acc: 0.5382 - val_loss: 0.7048 - val_acc: 0.5210 - 55s/epoch - 383ms/step
Epoch 3/100
144/144 - 49s - loss: 0.7023 - acc: 0.4965 - val_loss: 0.7319 - val_acc: 0.4790 - 49s/epoch - 343ms/step
Epoch 4/100
144/144 - 49s - loss: 0.6971 - acc: 0.5208 - val_loss: 0.6876 - val_acc: 0.5630 - 49s/epoch - 337ms/step
Epoch 5/100
144/144 - 49s - loss: 0.6980 - acc: 0.5417 - val_loss: 0.7644 - val_acc: 0.4622 - 49s/epoch - 342ms/step
Epoch 6/100
144/144 - 50s - loss: 0.7032 - acc: 0.5104 - val_loss: 0.6945 - val_acc: 0.5294 - 50s/epoch - 351ms/step
Epoch 7/100
144/144 - 52s - loss: 0.6911 - acc: 0.5069 - val_loss: 0.6850 - val_acc: 0.5882 - 52s/epoch - 364ms/step
Epoch 8/100
144/144 - 52s - loss: 0.6934 - acc: 0.5243 - val_loss: 0.6773 - val_acc: 0.5546 - 52s/epoch - 361ms/step
Epoch 9/100
144/144 - 53s - loss: 0.6866 - acc: 0.5729 - val_loss: 0.7048 - val_acc: 0.6303 - 53s/epoch - 371ms/step
Epoch 10/100
144/144 - 53s - loss: 0.6736 - acc: 0.5833 - val_loss: 0.6842 - val_acc: 0.6050 - 53s/epoch - 368ms/step
Epoch 11/100
144/144 - 54s - loss: 0.6872 - acc: 0.5590 - val_loss: 0.6872 - val_acc: 0.6134 - 54s/epoch - 376ms/step
Epoch 12/100
144/144 - 56s - loss: 0.6803 - acc: 0.5625 - val_loss: 0.8216 - val_acc: 0.5630 - 56s/epoch - 386ms/step
Epoch 13/100
144/144 - 55s - loss: 0.6861 - acc: 0.5486 - val_loss: 0.6642 - val_acc: 0.5882 - 55s/epoch - 383ms/step
Epoch 14/100
144/144 - 54s - loss: 0.6768 - acc: 0.6007 - val_loss: 0.7665 - val_acc: 0.5294 - 54s/epoch - 373ms/step
Epoch 15/100
144/144 - 56s - loss: 0.6845 - acc: 0.5625 - val_loss: 0.7927 - val_acc: 0.5210 - 56s/epoch - 391ms/step
Epoch 16/100
144/144 - 58s - loss: 0.6719 - acc: 0.5799 - val_loss: 0.6438 - val_acc: 0.6639 - 58s/epoch - 405ms/step
Epoch 17/100
144/144 - 57s - loss: 0.6780 - acc: 0.5764 - val_loss: 0.6916 - val_acc: 0.6723 - 57s/epoch - 398ms/step
Epoch 18/100
144/144 - 58s - loss: 0.6650 - acc: 0.6146 - val_loss: 0.6559 - val_acc: 0.6218 - 58s/epoch - 403ms/step
Epoch 19/100
144/144 - 57s - loss: 0.6543 - acc: 0.6181 - val_loss: 0.9822 - val_acc: 0.5126 - 57s/epoch - 397ms/step
Epoch 20/100
144/144 - 57s - loss: 0.6720 - acc: 0.5729 - val_loss: 0.7026 - val_acc: 0.5966 - 57s/epoch - 399ms/step
Epoch 21/100
144/144 - 58s - loss: 0.6707 - acc: 0.5903 - val_loss: 0.6813 - val_acc: 0.6555 - 58s/epoch - 403ms/step
Epoch 22/100
144/144 - 59s - loss: 0.6618 - acc: 0.5938 - val_loss: 0.8160 - val_acc: 0.5126 - 59s/epoch - 409ms/step
Epoch 23/100
144/144 - 59s - loss: 0.6583 - acc: 0.5903 - val_loss: 0.8234 - val_acc: 0.5966 - 59s/epoch - 413ms/step
Epoch 24/100
144/144 - 60s - loss: 0.6657 - acc: 0.5938 - val_loss: 0.6885 - val_acc: 0.4706 - 60s/epoch - 414ms/step
Epoch 25/100
144/144 - 61s - loss: 0.6481 - acc: 0.6458 - val_loss: 0.7492 - val_acc: 0.4958 - 61s/epoch - 423ms/step
Epoch 26/100
144/144 - 59s - loss: 0.6507 - acc: 0.6354 - val_loss: 0.6663 - val_acc: 0.6303 - 59s/epoch - 411ms/step
Epoch 27/100
144/144 - 60s - loss: 0.6442 - acc: 0.6215 - val_loss: 0.6698 - val_acc: 0.5630 - 60s/epoch - 420ms/step
Epoch 28/100
144/144 - 61s - loss: 0.6699 - acc: 0.6042 - val_loss: 0.7079 - val_acc: 0.5882 - 61s/epoch - 426ms/step
Epoch 29/100
144/144 - 61s - loss: 0.6514 - acc: 0.6181 - val_loss: 0.6649 - val_acc: 0.6050 - 61s/epoch - 422ms/step
Epoch 30/100
144/144 - 61s - loss: 0.6314 - acc: 0.6354 - val_loss: 1.0184 - val_acc: 0.5126 - 61s/epoch - 422ms/step
Epoch 31/100
144/144 - 61s - loss: 0.6409 - acc: 0.6458 - val_loss: 0.9145 - val_acc: 0.5294 - 61s/epoch - 423ms/step
Epoch 32/100
144/144 - 60s - loss: 0.6485 - acc: 0.6146 - val_loss: 0.9409 - val_acc: 0.5126 - 60s/epoch - 416ms/step
Epoch 33/100
144/144 - 59s - loss: 0.6263 - acc: 0.6424 - val_loss: 0.6993 - val_acc: 0.6218 - 59s/epoch - 412ms/step
Epoch 34/100
144/144 - 60s - loss: 0.6256 - acc: 0.6424 - val_loss: 0.7128 - val_acc: 0.5882 - 60s/epoch - 414ms/step
Epoch 35/100
144/144 - 59s - loss: 0.6277 - acc: 0.6354 - val_loss: 1.6613 - val_acc: 0.4790 - 59s/epoch - 411ms/step
Epoch 36/100
144/144 - 59s - loss: 0.6158 - acc: 0.6771 - val_loss: 0.7670 - val_acc: 0.5798 - 59s/epoch - 408ms/step
Epoch 37/100
144/144 - 59s - loss: 0.6181 - acc: 0.6493 - val_loss: 0.7200 - val_acc: 0.6303 - 59s/epoch - 409ms/step
Epoch 38/100
144/144 - 59s - loss: 0.6056 - acc: 0.6632 - val_loss: 0.8180 - val_acc: 0.5378 - 59s/epoch - 407ms/step
Epoch 39/100
144/144 - 59s - loss: 0.6224 - acc: 0.6285 - val_loss: 1.8640 - val_acc: 0.4790 - 59s/epoch - 407ms/step
Epoch 40/100
144/144 - 58s - loss: 0.6013 - acc: 0.7222 - val_loss: 1.2401 - val_acc: 0.4958 - 58s/epoch - 405ms/step
Epoch 41/100
144/144 - 58s - loss: 0.6099 - acc: 0.6736 - val_loss: 0.9245 - val_acc: 0.5042 - 58s/epoch - 406ms/step
Epoch 42/100
144/144 - 59s - loss: 0.5822 - acc: 0.6806 - val_loss: 0.7005 - val_acc: 0.6050 - 59s/epoch - 413ms/step
